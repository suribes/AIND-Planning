---
title: "AIND-Planning Heuristic Analysis"
author: "Sergio Uribe"
date: "17 april 2017"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Synopsis
Progression planning problems can be solved with graph searches such as breadth-first, depth-first, and A*, where the nodes of the graph are "states" and edges are "actions".  A "state" is the logical conjunction of all boolean ground "fluents", or state variables, that are possible for the problem using Propositional Logic. For example, we might have a problem to plan the transport of one cargo, C1, on a single available plane, P1, from one airport to another, SFO to JFK.
![State Space](./images/statespace.png)

Run uninformed planning searches for `air_cargo_p1`, `air_cargo_p2`, and `air_cargo_p3`; provide metrics on number of node expansions required, number of goal tests, time elapsed, and optimality of solution for each search algorithm. Include the result of at least three of these searches, including breadth-first and depth-first, in your write-up (`breadth_first_search` and `depth_first_graph_search`)

The questions to be asked are:

1. What was the best heuristic used in these problems?
2. Was it better than non-heuristic search planning methods for all problems?
3. Why or why not?

\pagebreak

# Initial state and goal

## Problem 1

Init(At(C1, SFO) $\wedge$ At(C2, JFK)$\\$
$\wedge$ At(P1, SFO) $\wedge$ At(P2, JFK)$\\$
$\wedge$ Cargo(C1) $\wedge$ Cargo(C2)$\\$
$\wedge$ Plane(P1) $\wedge$ Plane(P2)$\\$
$\wedge$ Airport(JFK) $\wedge$ Airport(SFO))$\\$$\\$
Goal(At(C1, JFK) $\wedge$ At(C2, SFO))$\\$

## Problem 2
Init(At(C1, SFO) $\wedge$ At(C2, JFK) $\wedge$ At(C3, ATL)$\\$
$\wedge$ At(P1, SFO) $\wedge$ At(P2, JFK) $\wedge$ At(P3, ATL)$\\$
$\wedge$ Cargo(C1) $\wedge$ Cargo(C2) $\wedge$ Cargo(C3)$\\$
$\wedge$ Plane(P1) $\wedge$ Plane(P2) $\wedge$ Plane(P3)$\\$
$\wedge$ Airport(JFK) $\wedge$ Airport(SFO) $\wedge$ Airport(ATL))$\\$$\\$
Goal(At(C1, JFK) $\wedge$ At(C2, SFO) $\wedge$ At(C3, SFO))$\\$

## Problem 3

Init(At(C1, SFO) $\wedge$ At(C2, JFK) $\wedge$ At(C3, ATL) $\wedge$ At(C4, ORD)$\\$
$\wedge$ At(P1, SFO) $\wedge$ At(P2, JFK)$\\$
$\wedge$ Cargo(C1) $\wedge$ Cargo(C2) $\wedge$ Cargo(C3) $\wedge$ Cargo(C4)$\\$
$\wedge$ Plane(P1) $\wedge$ Plane(P2)$\\$
$\wedge$ Airport(JFK) $\wedge$ Airport(SFO) $\wedge$ Airport(ATL) $\wedge$ Airport(ORD))$\\$$\\$
Goal(At(C1, JFK) $\wedge$ At(C3, JFK) $\wedge$ At(C2, SFO) $\wedge$ At(C4, SFO))$\\$


\pagebreak

# Optimal plans

## Problem 1
```
Load(C2, P2, JFK)
Load(C1, P1, SFO)
Fly(P2, JFK, SFO)
Unload(C2, P2, SFO)
Fly(P1, SFO, JFK)
Unload(C1, P1, JFK)

```


## Problem 2

```
Load(C2, P2, JFK)
Load(C1, P1, SFO)
Load(C3, P3, ATL)
Fly(P2, JFK, SFO)
Unload(C2, P2, SFO)
Fly(P1, SFO, JFK)
Unload(C1, P1, JFK)
Fly(P3, ATL, SFO)
Unload(C3, P3, SFO)

```


## Problem 3

```
Load(C1, P1, SFO)
Fly(P1, SFO, ATL)
Load(C3, P1, ATL)
Fly(P1, ATL, JFK)
Load(C2, P1, JFK)
Unload(C1, P1, JFK)
Unload(C3, P1, JFK)
Fly(P1, JFK, ORD)
Load(C4, P1, ORD)
Fly(P1, ORD, SFO)
Unload(C2, P1, SFO)
Unload(C4, P1, SFO)

```

\pagebreak

# Results
```{r, echo= FALSE}
library(data.table)
library(ggplot2)
library(scales)
library(knitr)
    
```

```{r}
results <- data.table(read.csv("results.csv", header= TRUE, sep = ","))
results <- results[, .(Problem_number = as.factor(Problem), Search_algorithm = as.factor(Search), Expansions, Goal_tests, New_nodes, Plan_length, Time_elapsed_ms  = as.integer(Time_elapsed * 1000)),]
results_m = melt(results, id.vars = c("Problem_number", "Search_algorithm"))
```

```{r, echo= FALSE}
col_names <- c("Problem", "Search", "Expansions", "Goal tests", "New nodes", "Plan length", "Time elapsed(ms)")
kable(results, caption = "Search Results", col.names = col_names)
```

```{r}
plot_results <- function(problem, search_algorithms) {
  p <- ggplot(results_m[Problem_number %in% c(as.character(problem))][Search_algorithm %in% search_algorithms],  aes(Search_algorithm, value)) + geom_bar(stat="identity") + facet_wrap(~variable, scales = "free_y", ncol= 1, dir = "h")  
  p <- p + scale_y_continuous(breaks= pretty_breaks()) + theme_minimal() + coord_fixed(2)
  p
}
```

## Non-heuristic search result metrics

```{r}
non_heuristic <- c("breadth_first", "depth_first", "greedy_best_first")
```

### Problem 1

```{r}
plot_results(1, non_heuristic)
```

The greedy_best_first search is more effective than breadth_first and both achieved an optimal solution. Depth_first is expected to not achieve an optimal solution.

### Problem 2

```{r}
plot_results(2, non_heuristic)
```
The breadth_first search is less effective than greedy_best_first, but it achieved an optimal solution. Neither depth_first nor greedy_best_first achieved an optimal solution.

### Problem 3

```{r}
plot_results(3, non_heuristic)
```
The breadth_first search is more effective than greedy_best_first, but it achieved an optimal solution. Neither depth_first nor greedy_best_first achieved an optimal solution.

## Heuristic search result metrics using A*

```{r}
heuristic <- c("ignore_preconditions", "pg_levelsum")
```

### Problem 1

```{r}
plot_results(1, heuristic)
```
The ignore_preconditions heuristic is more efficient than pg_levelsum and both achieved an optimal solution.

### Problem 2

```{r}
plot_results(2, heuristic)
```
The ignore_preconditions heuristic is more efficient than pg_levelsum and both achieved an optimal solution.

### Problem 3

```{r}
plot_results(3, heuristic)
```
The ignore_preconditions heuristic is more efficient than pg_levelsum and both achieved an optimal solution.

\pagebreak

# Conclussions

1. What was the best heuristic used in these problems?
The best heuristics used is the ignore preconditions heuristic.
2. Was it better than non-heuristic search planning methods for all problems?
It was not better for problem 1, but it was more effective in problems 2 and 3, and always achieving the optimal solution.
3. Why or why not?
The ignore preconditions heuristic does not need to calculate the costs of the individual goals. It uses the existing structure of the graph to calculate the remaining goals without being satisfied from the current position in the tree search. The negatives effects must be removed and the actions that achieve many goals are ignored.
This gives us a very good aproximation of the remaining cost [^fn1].

[^fn1]: Russsell, & S. Norvig, P. (2003). Planning with state-space search. Artificial Intelligence A Modern Approach $2_nd$ Ed. (pp. 386â€“387). USA: Prentice Hall.